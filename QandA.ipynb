{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled50.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7GjrAPW6ti8ntJKAf2hjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayasuryajsk/hugginface-Q-and-A/blob/main/QandA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AEdslVPYcjV"
      },
      "source": [
        "!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers\n",
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdQSKP98Y9GV"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import wikipedia as wiki\n",
        "import pprint as pp\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "\n",
        "class DocumentReader:\n",
        "    def __init__(self, pretrained_model_name_or_path='deepset/roberta-base-squad2 Has a model card'):\n",
        "        self.READER_PATH = pretrained_model_name_or_path\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
        "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
        "        self.max_len = self.model.config.max_position_embeddings\n",
        "        self.chunked = False\n",
        "\n",
        "    def tokenize(self, question, text):\n",
        "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "        if len(self.input_ids) > self.max_len:\n",
        "            self.inputs = self.chunkify()\n",
        "            self.chunked = True\n",
        "\n",
        "    def chunkify(self):\n",
        "        \"\"\" \n",
        "        Break up a long article into chunks that fit within the max token\n",
        "        requirement for that Transformer model. \n",
        "\n",
        "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
        "        [CLS] question tokens [SEP] context tokens [SEP].\n",
        "        \"\"\"\n",
        "\n",
        "        # create question mask based on token_type_ids\n",
        "        # value is 0 for question tokens, 1 for context tokens\n",
        "        qmask = self.inputs['token_type_ids'].lt(1)\n",
        "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
        "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
        "        # having to add an ending [SEP] token to the end\n",
        "\n",
        "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
        "        chunked_input = OrderedDict()\n",
        "        for k,v in self.inputs.items():\n",
        "            q = torch.masked_select(v, qmask)\n",
        "            c = torch.masked_select(v, ~qmask)\n",
        "            chunks = torch.split(c, chunk_size)\n",
        "            \n",
        "            for i, chunk in enumerate(chunks):\n",
        "                if i not in chunked_input:\n",
        "                    chunked_input[i] = {}\n",
        "\n",
        "                thing = torch.cat((q, chunk))\n",
        "                if i != len(chunks)-1:\n",
        "                    if k == 'input_ids':\n",
        "                        thing = torch.cat((thing, torch.tensor([102])))\n",
        "                    else:\n",
        "                        thing = torch.cat((thing, torch.tensor([1])))\n",
        "\n",
        "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
        "        return chunked_input\n",
        "\n",
        "    def get_answer(self):\n",
        "        if self.chunked:\n",
        "            answer = ''\n",
        "            for k, chunk in self.inputs.items():\n",
        "                answer_start_scores, answer_end_scores = self.model(**chunk)\n",
        "\n",
        "                answer_start = torch.argmax(answer_start_scores)\n",
        "                answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
        "                if ans != '[CLS]':\n",
        "                    answer += ans + \" / \"\n",
        "            return answer\n",
        "        else:\n",
        "            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n",
        "\n",
        "            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
        "            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
        "        \n",
        "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
        "                                              answer_start:answer_end])\n",
        "\n",
        "    def convert_ids_to_string(self, input_ids):\n",
        "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4qJrjcFf7JA"
      },
      "source": [
        "text = \"\"\" Stars are born as dense clouds of interstellar material collapse under their own gravity, spinning into flat discs that eventually spool into baby stars. Now, for the first time, hints of planet formation have been detected around a protostar so young, the cloud of leftover dust and gas is still collapsing into it, and the disc still forming.\n",
        "\n",
        "This is the earliest detection of such structures in a protostellar ring, and it suggests that planet formation starts earlier than we thought, before the nascent system is even 500,000 years old.\n",
        "\n",
        "The young protostar is called IRS 63, and it's 470 light-years away in the Rho Ophiuchi star formation region - a stellar nursery where the dust is thick enough to form the spinning clumps that will eventually form stars.\n",
        "\n",
        "IRS 63 is in class I of the star formation process, less than half a million years old. It's past the main accretion phase, and has most of its final mass; shining brightly in millimetre wavelengths, it's also one of the brightest protostars of its class.\n",
        "\n",
        "Additionally, IRS 63 has a large disc, extending out to around 50 astronomical units. These properties, along with its proximity, make the object an excellent target for studying star and planet formation.\n",
        "\n",
        "rho ophiuchi\n",
        "The Rho Ophiuchi star-forming region. (ESO/Digitized Sky Survey 2)\n",
        "\n",
        "Using the Atacama Large Millimeter/submillimeter Array in Chile - a radio telescope with an excellent track record of detecting early planet formation - a team led by astronomer Dominique Segura-Cox of the Max Planck Institute for Extraterrestrial Physics in Germany took a closer look at the star and the dusty cloud around it.\n",
        "\n",
        "There, in the swirling disc, the team found a surprise: two dark concentric gaps centred around the protostar - what astronomers take to be a sign of planet formation.\n",
        "\n",
        "Planet formation is a poorly understood process. The most popular model is core accretion - grains of dust in the disc gradually accumulating, first sticking together electrostatically, then gravitationally as the body grows larger and larger. As this occurs, the protoplanet hoovers up all the material along its orbital path, creating a gap in the circumstellar disc.\n",
        "\n",
        "Such gaps have been detected in almost all discs we've imaged with sufficiently high resolution. But there is a big problem with the model - it takes a very long time for planets to form that way, and protostellar discs older than about 1 million years old don't seem to have enough material to form the known exoplanet population.\n",
        "\n",
        "Astronomers have found over 35 class II protostellar systems around the age of 1 million years that have lost their large dust clouds, but still have protostellar discs and sport pronounced gaps therein. The fact they have such well-developed gaps at just 1 million years old, suggests planet formation process is well underway by the time stars are of this age.\n",
        "\n",
        "If the structures detected by Segura-Cox and her team are created by planets, it would support this idea, and offer a solution to the problem of missing mass in the protostellar disc.\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRtUxl3zahG0",
        "outputId": "acc41704-224f-4ac3-c647-66db6921416e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "questions = [\n",
        "    'when will planet formation start?'\n",
        "]\n",
        "\n",
        "reader = DocumentReader(\"deepset/bert-large-uncased-whole-word-masking-squad2\")\n",
        "\n",
        "# if you trained your own model using the training cell earlier, you can access it with this:\n",
        "#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"Question: {question}\")\n",
        "    results = wiki.search(question)\n",
        "\n",
        "    page = wiki.page(results[0])\n",
        "    #print(f\"Top wiki result: {page}\")\n",
        "\n",
        "    #text = \n",
        "\n",
        "    reader.tokenize(question, text)\n",
        "    print(f\"Answer: {reader.get_answer()}\")\n",
        "    print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question: when will planet formation start?\n",
            "Answer: before the nascent system is even 500 , 000 years old / by the time stars are of this age / \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlwYlJNKkANU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXBhsOpBe3Mc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}